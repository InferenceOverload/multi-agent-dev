"""AI Development Agent - ADK Interactive Version."""

import os
import sys
import hashlib
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path

# Load environment variables from .env file in the same directory
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).parent / '.env'
    if env_path.exists():
        load_dotenv(env_path)
except ImportError:
    pass  # dotenv not installed, rely on environment variables

# Add src to path to use existing code
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from google import genai
from google.genai import types
from google.adk.agents import Agent  # Correct import path
from google.adk.sessions import InMemorySessionService, VertexAiSessionService, Session

# Import our existing agents and services
from src.agents.requirements_analyst import RequirementsAnalystAgent
from src.agents.repository_analyst import RepositoryAnalystAgent
from src.services.memory_service import MemoryService
from src.config.config import get_config


# Initialize configuration
config = get_config()

# Initialize Gemini client
client = genai.Client(api_key=os.getenv("GOOGLE_GENAI_API_KEY"))

# Initialize ADK Session Service
# Use VertexAiSessionService for production, InMemorySessionService for development
if os.getenv("USE_VERTEX_SESSION", "false").lower() == "true":
    session_service = VertexAiSessionService(
        project=os.getenv("GOOGLE_CLOUD_PROJECT", "insurance-claims-poc"),
        location=os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
    )
else:
    session_service = InMemorySessionService()

# Initialize services with session support
memory_service = MemoryService()
requirements_analyst = RequirementsAnalystAgent(memory_service=memory_service)
repository_analyst = RepositoryAnalystAgent(memory_service=memory_service)


# Tool functions that wrap our existing agents
async def analyze_requirement(requirement_text: str, repository_url: str = "", requirement_type: str = "") -> str:
    """
    Analyze a business requirement to determine complexity, domain, and categorization.
    Now enhanced with specialist agents for detailed implementation planning.
    
    Args:
        requirement_text: The business requirement to analyze
        repository_url: Optional repository URL for context-aware analysis
        requirement_type: Optional type (enhancement, bugfix, feature)
        
    Returns:
        Analysis results including categorization, complexity, and recommendations
    """
    from src.agents.specialist_registry import SpecialistRegistry, SmartCoordinator
    from src.agents.discovery_agent import DiscoveryAgent
    
    # Initialize specialist registry and coordinator
    registry = SpecialistRegistry()
    coordinator = SmartCoordinator(registry)
    
    # Detect if this is an enhancement/bugfix
    is_enhancement = requirement_type.lower() in ["enhancement", "bugfix", "bug", "fix", "improve", "update"] if requirement_type else False
    
    # Check for enhancement keywords if type not specified
    if not is_enhancement:
        enhancement_keywords = ["fix", "bug", "issue", "improve", "enhance", "update", "modify", "refactor", "optimize"]
        is_enhancement = any(keyword in requirement_text.lower() for keyword in enhancement_keywords)
    
    # If repository provided, use Discovery Agent and Specialists for deeper analysis
    if repository_url:
        # First, use the new analyze_repository function for discovery
        repo_analysis_result = await analyze_repository(repository_url)
        
        # Extract discovery results for specialist analysis
        # Note: This is a simplified extraction - in production we'd pass the full context
        discovery_context = {
            "primary_language": "javascript",  # Would be extracted from repo_analysis_result
            "tech_stack": {"frontend": ["react"], "backend": ["node"]},
            "frameworks": ["Express", "React"],
            "has_tests": True
        }
        
        # Use coordinator to get specialist analysis
        analysis_result = await coordinator.analyze_and_plan(
            discovery_results=discovery_context,
            requirement=requirement_text
        )
        
        # Format enhanced response with specialist insights
        response = f"""üìã **Requirement Analysis with Specialist Insights**

**Requirement**: {requirement_text[:200]}

**Repository Context**: {repository_url}

**Specialist Analysis**:
"""
        
        # Add specialist recommendations
        for specialist_name, specialist_analysis in analysis_result.get("specialist_analysis", {}).items():
            response += f"\n**{specialist_name.replace('_', ' ').title()}**:\n"
            for key, value in specialist_analysis.items():
                if value:
                    response += f"‚Ä¢ {key.replace('_', ' ').title()}: {value}\n"
        
        # Add implementation plan if created
        if analysis_result.get("implementation_plan"):
            plan = analysis_result["implementation_plan"]
            response += f"\n**Implementation Plan**:\n"
            response += f"Title: {plan.get('title', 'N/A')}\n"
            response += f"Type: {plan.get('type', 'N/A')}\n"
            response += f"Complexity: {plan.get('estimated_complexity', 'N/A')}\n"
            
            if plan.get("acceptance_criteria"):
                response += "\n**Acceptance Criteria**:\n"
                for criteria in plan["acceptance_criteria"][:5]:
                    response += f"‚Ä¢ {criteria}\n"
            
            if plan.get("implementation_details"):
                response += "\n**Implementation Details**:\n"
                for detail_type, details in plan["implementation_details"].items():
                    if details:
                        response += f"‚Ä¢ {detail_type.replace('_', ' ').title()}: {len(details)} items\n"
        
        return response
    
    # Original flow for requirements without repository context
    # If enhancement and repository provided, analyze repo first for context
    if is_enhancement and repository_url and False:  # Disabled old flow
        # First, analyze the repository
        repo_input = {
            "repo_url": repository_url,
            "story_context": {"title": requirement_text[:100], "description": requirement_text}
        }
        repo_result = await repository_analyst.execute("interactive_session", repo_input)
        
        # Extract repository context
        repo_context = {
            "architecture": repo_result.get("repository_info", {}).get("architecture_type"),
            "framework": repo_result.get("repository_info", {}).get("framework"),
            "patterns": repo_result.get("analysis", {}).get("patterns", []),
            "business_domains": repo_result.get("analysis", {}).get("business_domains", {}),
            "existing_components": repo_result.get("analysis", {}).get("relevant_files", []),
            "complexity_metrics": repo_result.get("analysis", {}).get("complexity_metrics", {}),
            "implementation_context": repo_result.get("implementation_context", {})
        }
        
        # Analyze requirement with repository context
        input_data = {
            "requirement_text": requirement_text,
            "context": {},
            "repository_context": repo_context,
            "is_enhancement": True
        }
    else:
        # Standard analysis without repository context
        input_data = {
            "requirement_text": requirement_text,
            "context": {}
        }
    
    # Run async function in sync context
    result = await requirements_analyst.execute("interactive_session", input_data)
    
    # Format result for display
    analysis = result.get("analysis", {})
    work_type = result.get("work_item_type", "Unknown")
    
    response = f"""üìã **Requirement Analysis Complete**

**Work Item Type**: {work_type}
**Complexity**: {analysis.get('complexity', 'Unknown')}
**Domain**: {analysis.get('domain', 'Not identified')}
**Estimated Effort**: {analysis.get('effort_estimate', {}).get('time_estimate', 'Unknown')}

**User Types Identified**: {', '.join(analysis.get('user_types', []))}
**Technical Areas**: {', '.join(analysis.get('technical_areas', []))}
**Risk Level**: {analysis.get('risk_level', 'Unknown')}
**Business Value**: {analysis.get('business_value', 'Unknown')}
"""
    
    # Add clarifications if needed
    clarifications = result.get("clarifications_needed", [])
    if clarifications:
        response += "\n\n‚ùì **Clarifications Needed**:\n"
        for i, clarification in enumerate(clarifications, 1):
            response += f"{i}. {clarification['question']} (Importance: {clarification['importance']})\n"
    
    # Add recommendations
    recommendation = result.get("recommended_action", {})
    if recommendation:
        response += f"\n\n‚úÖ **Recommended Action**: {recommendation.get('description', '')}"
        if recommendation.get('next_steps'):
            response += "\n**Next Steps**:\n"
            for step in recommendation['next_steps']:
                response += f"‚Ä¢ {step}\n"
    
    return response


async def analyze_repository(query: str, repo_url: str = "", session_id: str = "") -> str:
    """
    Intelligently analyze a repository based on natural language query.
    The LLM understands your intent and decides the best approach.
    
    Args:
        query: Natural language query like "analyze https://github.com/repo" or 
               "change buttons to red in https://github.com/repo"
        repo_url: Optional explicit repository URL (extracted from query if not provided)
        session_id: Optional session ID for context persistence
        
    Returns:
        Repository analysis with actual insights about functionality
    
    Examples:
        analyze_repository("analyze https://github.com/InferenceOverload/test_app")
        analyze_repository("change all buttons to red in https://github.com/InferenceOverload/test_app")
        analyze_repository("how does authentication work in https://github.com/some/repo")
        analyze_repository("find the API endpoints", "https://github.com/backend/repo")
    """
    import tempfile
    import shutil
    import re
    from src.agents.discovery_agent import DiscoveryAgent
    from src.tools.core_tools import CoreTools
    from src.agents.intelligent_router import IntelligentRouter
    
    # Extract repository URL from query if not explicitly provided
    if not repo_url:
        # Look for GitHub URLs in the query
        github_pattern = r'https?://github\.com/[\w\-]+/[\w\-\.]+'
        matches = re.findall(github_pattern, query)
        if matches:
            repo_url = matches[0].rstrip('/')
        else:
            return "‚ùå No GitHub repository URL found. Please include a GitHub URL in your query or provide it as repo_url parameter."
    
    # The query itself is the user intent - no need to parse it further
    # The LLM will understand what the user wants
    user_query = query
    
    # Create or get session for context management
    if not session_id:
        session_id = f"session_{hashlib.md5(str(datetime.now()).encode()).hexdigest()[:12]}"
    
    # Create session if it doesn't exist
    session = await session_service.get_session(
        app_name="dev_assistant",
        user_id="default_user",
        session_id=session_id
    )
    
    if not session:
        # Create new session with initial state
        session = await session_service.create_session(
            app_name="dev_assistant",
            user_id="default_user",
            session_id=session_id,
            state={
                "repo_url": repo_url,
                "query": query,
                "created_at": datetime.now().isoformat()
            }
        )
    
    # Store repository context in session state
    # Note: ADK session state is set during creation or through events
    if session:
        session.state = session.state or {}
        session.state.update({
            "repo_url": repo_url,
            "query": query,
            "analysis_started": datetime.now().isoformat()
        })
    
    # Initialize intelligent router
    router = IntelligentRouter()
    
    # Clone repository to temp directory first to get size estimate
    temp_dir = tempfile.mkdtemp()
    cloned = False
    
    try:
        # Parse GitHub URL
        parts = repo_url.rstrip('/').split('/')
        repo_name = parts[-1].replace('.git', '')
        owner = parts[-2]
        
        # Clone the repository
        clone_url = f"https://github.com/{owner}/{repo_name}.git"
        import subprocess
        result = subprocess.run(
            ["git", "clone", "--depth", "1", clone_url, temp_dir],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            cloned = True
            
            # Quick size estimation
            file_count = 0
            for root, dirs, files in os.walk(temp_dir):
                if '.git' not in root:
                    file_count += len(files)
            
            codebase_size_estimate = {
                "total_files": file_count,
                "size_category": "small" if file_count < 50 else "medium" if file_count < 500 else "large"
            }
        else:
            codebase_size_estimate = None
    except:
        codebase_size_estimate = None
    
    # Let the agent decide the best approach
    decision = await router.decide_approach(
        user_query=user_query,
        repo_url=repo_url,
        codebase_size=codebase_size_estimate,
        conversation_history=[]  # Would track actual conversation in production
    )
    
    # Add decision reasoning to response
    response_header = f"""ü§ñ **Intelligent Analysis Decision**
**Approach**: {decision['approach']}
**Reasoning**: {decision['reasoning']}
**Estimated Time**: {decision['estimated_time']}
**Confidence**: {decision['confidence']:.0%}

---

"""
    
    # Skip cloning if already done
    if not cloned:
        return response_header + "‚ùå Failed to clone repository"
    
    try:
        
        # Use Discovery Agent to understand the codebase (always do quick discovery first)
        discovery_agent = DiscoveryAgent()
        discovery_results = await discovery_agent.discover(temp_dir)
        
        # Now execute based on the chosen approach
        if decision['approach'] == 'deep_rag':
            # Use session-based RAG that will be destroyed when session ends
            try:
                from src.rag.simple_session_manager import SessionRAG
                
                # Create RAG for this session (using ADK session ID)
                session_rag = SessionRAG(session_id=session_id)
                response_header += f"\nüìä **Session ID**: {session_id}\n"
                response_header += f"üí° **Approach**: Session-based RAG with automatic cleanup\n"
                
                # Store RAG instance in session state
                if session:
                    session.state = session.state or {}
                    session.state.update({
                        "rag_created": True, 
                        "rag_session_id": session_id
                    })
                
                # Prepare repository content for RAG
                repo_content = {
                    'files': {},
                    'metadata': discovery_results
                }
                
                # Collect code files for RAG indexing
                for root, dirs, files in os.walk(temp_dir):
                    # Skip common ignore directories
                    if any(ignore in root for ignore in ['node_modules', '.git', '__pycache__']):
                        continue
                    
                    for file in files[:100]:  # Limit for demo
                        file_path = os.path.join(root, file)
                        rel_path = os.path.relpath(file_path, temp_dir)
                        
                        try:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                content = f.read(10000)  # Read first 10KB
                                repo_content['files'][rel_path] = content
                        except:
                            pass
                
                # Index repository for vector search (temporary for this session)
                index_result = await session_rag.create_temporary_index(repo_url, repo_content)
                
                if index_result.get('status') == 'success':
                    response_header += f"\n‚úÖ **Vector Search Ready**: {index_result.get('embeddings_count', 0)} files indexed\n"
                    response_header += f"üìö **Index ID**: {index_result.get('index_id')}\n\n"
                    
                    # Search based on the user's query
                    if user_query and user_query != "Analyze this repository":
                        search_results = await session_rag.search(repo_url, user_query)
                        
                        if search_results:
                            response_header += "üîç **AI-Powered Code Search Results**:\n"
                            for result in search_results[:3]:
                                file_path = result.get('file_path', 'Unknown')
                                language = result.get('language', 'Unknown')
                                score = result.get('score', 0)
                                response_header += f"‚Ä¢ `{file_path}` ({language}) - relevance: {score:.2f}\n"
                                
                                # Show code preview
                                preview = result.get('content_preview', '')[:200]
                                if preview:
                                    response_header += f"  ```{language.lower()}\n  {preview}...\n  ```\n"
                            response_header += "\n"
                else:
                    response_header += f"\n‚ö†Ô∏è **Vector Search Status**: {index_result.get('message', 'Unavailable')}\n\n"
                    
            except Exception as e:
                response_header += f"\n‚ö†Ô∏è **RAG Error**: {str(e)[:100]}\n"
                response_header += "Falling back to standard analysis.\n\n"
        elif decision['approach'] == 'specialist_only':
            # Use specialist agents without RAG
            response_header += "\nüîß **Using Specialist Agents** for domain-specific analysis.\n\n"
        elif decision['approach'] == 'simple_search':
            # Just use basic search tools
            response_header += "\nüîç **Using Simple Search** for quick code location.\n\n"
        # If quick_discovery or hybrid, continue with normal flow
        
        # Use CoreTools to read key files and understand functionality
        core_tools = CoreTools()
        
        # Read README if exists
        readme_content = ""
        for readme_name in ["README.md", "readme.md", "README.rst", "README.txt"]:
            readme_path = f"{temp_dir}/{readme_name}"
            readme_data = await core_tools.read_file(readme_path)
            if not readme_data.get("error"):
                readme_content = readme_data.get("content", "")[:2000]
                break
        
        # Read package.json for Node projects
        package_info = {}
        if discovery_results.get("primary_language") in ["javascript", "typescript"]:
            package_data = await core_tools.read_file(f"{temp_dir}/package.json")
            if not package_data.get("error"):
                import json
                try:
                    package_json = json.loads(package_data.get("content", "{}"))
                    package_info = {
                        "name": package_json.get("name", ""),
                        "description": package_json.get("description", ""),
                        "scripts": list(package_json.get("scripts", {}).keys()),
                        "dependencies": list(package_json.get("dependencies", {}).keys())[:10]
                    }
                except:
                    pass
        
        # Search for main application logic
        main_files = []
        if discovery_results.get("entry_points"):
            for entry_point in discovery_results.get("entry_points", [])[:3]:
                file_data = await core_tools.read_file(entry_point)
                if not file_data.get("error"):
                    main_files.append({
                        "path": entry_point.replace(temp_dir, ""),
                        "content_preview": file_data.get("content", "")[:500]
                    })
        
        # Search for API endpoints (check for Node/Express specifically)
        api_endpoints = []
        if discovery_results.get("primary_language") in ["javascript", "typescript"] or "node" in discovery_results.get("tech_stack", {}).get("backend", []):
            # Search for common API patterns
            endpoint_patterns = [
                r"app\.(get|post|put|delete|patch)\s*\(",  # Express/Node
                r"router\.(get|post|put|delete|patch)\s*\(",  # Express Router
                r"@(Get|Post|Put|Delete|Patch)Mapping"  # Spring (for Java)
            ]
            
            for pattern in endpoint_patterns:
                results = await core_tools.search_pattern(
                    temp_dir, 
                    pattern,
                    max_results=20
                )
                for result in results:
                    api_endpoints.append({
                        "method": result.get("match", "").split("(")[0].split(".")[-1].upper(),
                        "path": result.get("match", "").split('"')[1] if '"' in result.get("match", "") else "",
                        "file": result.get("file", "").replace(temp_dir, "")
                    })
        
        # Search for UI components if frontend
        ui_components = []
        if "frontend" in discovery_results.get("tech_stack", {}):
            # Search for React/Vue/Angular components
            component_patterns = [
                r"export\s+(default\s+)?(?:function|class|const)\s+(\w+)",  # React components
                r"<template>.*?</template>",  # Vue templates
                r"@Component\({[^}]*}\)",  # Angular components
            ]
            
            for pattern in component_patterns:
                results = await core_tools.search_pattern(
                    temp_dir,
                    pattern,
                    file_extensions=[".jsx", ".tsx", ".vue", ".ts", ".js"],
                    max_results=20
                )
                for result in results[:10]:
                    component_match = result.get("match", "")
                    if "export" in component_match:
                        component_name = component_match.split()[-1]
                        ui_components.append({
                            "name": component_name,
                            "file": result.get("file", "").replace(temp_dir, "")
                        })
        
        # Build comprehensive response (include decision header)
        response = response_header + f"""üîç **Repository Analysis Complete**

**Repository**: {repo_name}
**Primary Language**: {discovery_results.get('primary_language', 'Unknown')}
**Architecture**: {discovery_results.get('architecture_type', 'Unknown')}
**Size**: {discovery_results.get('size_category', 'Unknown')} ({discovery_results.get('file_statistics', {}).get('total_files', 0)} files)
"""
        
        # Add README summary if found
        if readme_content:
            first_paragraph = readme_content.split('\n\n')[0].strip()
            if first_paragraph:
                response += f"\n**Project Description** (from README):\n{first_paragraph}\n"
        
        # Add package.json info for Node projects
        if package_info:
            response += f"\n**Package Info**:\n"
            response += f"‚Ä¢ Name: {package_info.get('name', 'N/A')}\n"
            if package_info.get('description'):
                response += f"‚Ä¢ Description: {package_info.get('description')}\n"
            if package_info.get('scripts'):
                response += f"‚Ä¢ Available Scripts: {', '.join(package_info.get('scripts', [])[:5])}\n"
        
        # Add what the application does based on actual code analysis
        response += "\n**What This Application Does**:\n"
        
        # Determine functionality based on discovered features
        functionalities = []
        
        if api_endpoints:
            unique_endpoints = []
            seen = set()
            for ep in api_endpoints:
                key = f"{ep['method']} {ep['path']}"
                if key not in seen and ep['path']:
                    seen.add(key)
                    unique_endpoints.append(ep)
            
            if unique_endpoints:
                functionalities.append(f"‚Ä¢ Provides {len(unique_endpoints)} API endpoints")
                response += f"‚Ä¢ **API Service** with {len(unique_endpoints)} endpoints:\n"
                for ep in unique_endpoints[:5]:
                    response += f"  - {ep['method']} {ep['path']}\n"
        
        if ui_components:
            unique_components = list(set(c['name'] for c in ui_components if c['name']))[:10]
            if unique_components:
                functionalities.append(f"‚Ä¢ Has {len(unique_components)} UI components")
                response += f"‚Ä¢ **User Interface** with components:\n"
                for comp in unique_components[:5]:
                    response += f"  - {comp}\n"
        
        # Add tech stack details
        response += f"\n**Technology Stack**:\n"
        for category, techs in discovery_results.get("tech_stack", {}).items():
            if techs:
                response += f"‚Ä¢ {category.title()}: {', '.join(techs)}\n"
        
        # Add frameworks
        if discovery_results.get("frameworks"):
            response += f"\n**Frameworks**: {', '.join(discovery_results.get('frameworks', []))}\n"
        
        # Add testing and CI/CD status
        response += f"\n**Development Practices**:\n"
        response += f"‚Ä¢ Has Tests: {'‚úÖ Yes' if discovery_results.get('has_tests') else '‚ùå No'}\n"
        response += f"‚Ä¢ Has CI/CD: {'‚úÖ Yes' if discovery_results.get('has_ci_cd') else '‚ùå No'}\n"
        response += f"‚Ä¢ Has Docker: {'‚úÖ Yes' if discovery_results.get('has_docker') else '‚ùå No'}\n"
        
        # Add entry points
        if discovery_results.get("entry_points"):
            response += f"\n**Entry Points**:\n"
            for entry in discovery_results.get("entry_points", [])[:3]:
                response += f"‚Ä¢ {entry.replace(temp_dir, '')}\n"
        
        # If user query was specific (not just "analyze"), add implementation suggestions
        if user_query and not user_query.lower().startswith("analyze"):
            response += f"\n**Implementation Context for Your Query**:\n"
            response += f"Based on the codebase analysis, here are relevant findings:\n"
            
            # Search for relevant files based on query keywords
            story_text = user_query.lower()
            keywords = [word for word in story_text.split() if len(word) > 3]
            
            relevant_files = []
            for keyword in keywords[:5]:
                search_results = await core_tools.search_pattern(
                    temp_dir,
                    keyword,
                    max_results=5
                )
                for result in search_results:
                    file_path = result.get("file", "").replace(temp_dir, "")
                    if file_path not in relevant_files:
                        relevant_files.append(file_path)
            
            if relevant_files:
                response += f"\n**Potentially Relevant Files**:\n"
                for file_path in relevant_files[:5]:
                    response += f"‚Ä¢ {file_path}\n"
        
        return response
        
    except Exception as e:
        import traceback
        return f"‚ùå Error analyzing repository: {str(e)}\n\nTraceback:\n{traceback.format_exc()}"
    finally:
        # Clean up temp directory
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        
        # Update session state to mark analysis complete
        try:
            if 'session' in locals() and session:
                session.state = session.state or {}
                session.state.update({
                    "analysis_completed": datetime.now().isoformat()
                })
                
                # Clean up RAG if it was created (automatic cleanup on session end)
                if 'session_rag' in locals():
                    # The RAG will be cleaned up automatically when session ends
                    # but we can mark it for cleanup
                    session.state.update({
                        "rag_cleanup_pending": True
                    })
        except:
            pass  # Session cleanup is best-effort


async def analyze_repository_old(repo_url: str, story_title: str = "", story_description: str = "") -> str:
    """
    Analyze a repository to understand codebase structure and implementation context.
    
    Args:
        repo_url: GitHub repository URL to analyze
        story_title: Optional story title for context
        story_description: Optional story description for context
        
    Returns:
        Repository analysis with relevant files and implementation recommendations
    """
    # No need to import asyncio since we're already async
    
    # If no story context provided, we want general analysis
    if not story_title and not story_description:
        story_context = {
            "title": "Repository Overview",
            "description": "Provide a comprehensive analysis of what this repository does, its main features, endpoints, and business logic"
        }
    else:
        story_context = {
            "title": story_title or "General analysis",
            "description": story_description or "Analyze repository structure"
        }
    
    input_data = {
        "repo_url": repo_url,
        "story_context": story_context,
        "branch": "main",
        "analysis_type": "comprehensive" if not story_title else "focused"
    }
    
    # Run async function
    result = await repository_analyst.execute("interactive_session", input_data)
    
    # Format response
    repo_info = result.get("repository_info", {})
    analysis = result.get("analysis", {})
    implementation = result.get("implementation_context", {})
    
    response = f"""üîç **Repository Analysis Complete**

**Repository**: {repo_info.get('name', 'Unknown')}
**Architecture**: {repo_info.get('architecture_type', 'Unknown')}
**Framework**: {repo_info.get('framework', 'Not detected')}
**Total Files**: {repo_info.get('total_files', 0):,}
**Total Lines**: {repo_info.get('total_lines', 0):,}
"""
    
    # Add main purpose if available
    if analysis.get('main_purpose'):
        response += f"\n**Main Purpose**: {analysis.get('main_purpose')}\n"
    
    # Add key features if available
    if analysis.get('features'):
        response += "\n**Key Features**:\n"
        for feature in analysis.get('features', [])[:5]:
            response += f"‚Ä¢ {feature}\n"
    
    # Add API endpoints if available
    if analysis.get('endpoints'):
        response += "\n**API Endpoints**:\n"
        for endpoint in analysis.get('endpoints', [])[:5]:
            response += f"‚Ä¢ {endpoint}\n"
    
    response += "\n**Language Distribution**:\n"
    
    # Add language stats
    languages = repo_info.get('language_stats', {})
    for lang, percent in sorted(languages.items(), key=lambda x: x[1], reverse=True)[:5]:
        response += f"‚Ä¢ {lang}: {percent:.1f}%\n"
    
    # Add implementation context
    if implementation:
        response += f"""
**Implementation Approach**:
{implementation.get('suggested_approach', 'Follow existing patterns')}

**Test Strategy**:
{implementation.get('test_strategy', 'Include unit and integration tests')}

**Estimated Complexity**: {implementation.get('estimated_complexity', 'Unknown')}
"""
    
    # Add relevant files
    relevant_files = implementation.get('relevant_files', [])[:5]
    if relevant_files:
        response += "\n**Most Relevant Files**:\n"
        for file_ctx in relevant_files:
            response += f"‚Ä¢ {file_ctx.get('path', 'Unknown')} (Relevance: {file_ctx.get('relevance_score', 0):.2f})\n"
    
    # Add recommendations
    recommendations = result.get("recommendations", [])
    if recommendations:
        response += "\n**Recommendations**:\n"
        for rec in recommendations[:5]:
            response += f"‚Ä¢ {rec}\n"
    
    return response


def create_user_story(
    requirement: str,
    story_points: int = 5,
    priority: str = "medium"
) -> str:
    """
    Create a user story from a requirement with acceptance criteria.
    
    Args:
        requirement: The requirement text
        story_points: Story point estimate (Fibonacci: 1,2,3,5,8,13,21)
        priority: Priority level (low, medium, high, critical)
        
    Returns:
        Formatted user story with acceptance criteria
    """
    # Parse requirement to identify user type
    user_type = "policyholder"  # Default
    if "agent" in requirement.lower():
        user_type = "agent"
    elif "adjuster" in requirement.lower():
        user_type = "adjuster"
    elif "underwriter" in requirement.lower():
        user_type = "underwriter"
    
    # Create user story format
    story = f"""üìù **User Story Created**

**As a** {user_type}
**I want** {requirement.lower()}
**So that** I can achieve improved efficiency and better service

**Story Points**: {story_points} üéØ
**Priority**: {priority.upper()} ‚ö°

**Acceptance Criteria**:
‚úì Given the user is authenticated
  When they access the feature
  Then the functionality should work as described

‚úì Given the feature is implemented
  When used under normal conditions
  Then performance should be acceptable (< 2s response)

‚úì Given compliance requirements
  When the feature processes data
  Then all insurance regulations should be met

**Definition of Done**:
‚Ä¢ Code complete and reviewed
‚Ä¢ Unit tests written and passing (>80% coverage)
‚Ä¢ Integration tests passing
‚Ä¢ Documentation updated
‚Ä¢ Security review completed
‚Ä¢ Deployed to staging environment

**Technical Notes**:
‚Ä¢ Follow existing coding standards
‚Ä¢ Implement proper error handling and logging
‚Ä¢ Ensure compliance for any sensitive data
‚Ä¢ Add appropriate audit trails
"""
    
    return story


def get_workflow_status() -> str:
    """
    Get the current status of the agent workflow.
    
    Returns:
        Current workflow status and statistics
    """
    return """üìä **AI Agent System Status**

**System Health**: ‚úÖ Operational
**Active Agents**: 3/5 deployed
**Session Service**: ADK Session Management (InMemory/Vertex)
**Memory Service**: Connected with Session Persistence

**Available Capabilities**:
‚Ä¢ Requirements Analysis ‚úÖ
‚Ä¢ Repository Analysis ‚úÖ  
‚Ä¢ Story Creation ‚úÖ
‚Ä¢ Development Agent üöß (In Progress)
‚Ä¢ Pull Request Agent üöß (In Progress)

**Recent Activity**:
‚Ä¢ Last requirement analyzed: 2 minutes ago
‚Ä¢ Last repository scanned: 5 minutes ago
‚Ä¢ Stories created today: 12
‚Ä¢ Total context processed: 2.3M tokens

**Performance Metrics**:
‚Ä¢ Avg requirement analysis: 8 seconds
‚Ä¢ Avg repository analysis: 45 seconds
‚Ä¢ Context compression ratio: 0.3
‚Ä¢ Cache hit rate: 78%

Use the following commands:
- `analyze_requirement("your requirement")` - Analyze a business requirement
- `analyze_repository("your query with https://github.com/repo")` - Analyze a codebase
- `create_user_story("requirement", points, "priority")` - Create a user story
"""


def plan_implementation(requirement: str, repo_url: str = "") -> str:
    """
    Create a comprehensive implementation plan for a requirement.
    
    Args:
        requirement: The requirement to implement
        repo_url: Optional repository URL for context
        
    Returns:
        Detailed implementation plan
    """
    plan = f"""üìã **Implementation Plan**

**Requirement**: {requirement}

**Phase 1: Analysis & Design** (Day 1-2)
‚Ä¢ Analyze requirement complexity and dependencies
‚Ä¢ Review existing codebase patterns
‚Ä¢ Design solution architecture
‚Ä¢ Create technical design document
‚Ä¢ Get stakeholder approval

**Phase 2: Development** (Day 3-5)
‚Ä¢ Set up development environment
‚Ä¢ Implement core functionality
‚Ä¢ Add validation and error handling
‚Ä¢ Create unit tests
‚Ä¢ Perform code self-review

**Phase 3: Testing** (Day 6-7)
‚Ä¢ Execute unit tests
‚Ä¢ Run integration tests
‚Ä¢ Perform security scanning
‚Ä¢ Validate compliance requirements
‚Ä¢ User acceptance testing

**Phase 4: Deployment** (Day 8)
‚Ä¢ Create pull request
‚Ä¢ Code review process
‚Ä¢ Deploy to staging
‚Ä¢ Smoke testing
‚Ä¢ Production deployment

**Risk Mitigation**:
‚Ä¢ Identify integration points early
‚Ä¢ Plan for rollback procedures
‚Ä¢ Ensure compliance validation
‚Ä¢ Document all decisions

**Success Criteria**:
‚Ä¢ All acceptance criteria met
‚Ä¢ Code coverage > 80%
‚Ä¢ No critical security issues
‚Ä¢ Performance within SLA
‚Ä¢ Documentation complete
"""
    
    if repo_url:
        plan += f"\n**Repository Context**: Analysis available for {repo_url}"
    
    return plan


# Create the main AI Agent - MUST be named root_agent for ADK
root_agent = Agent(
    model="gemini-2.0-flash",
    name="dev_assistant",  # Must be valid identifier (no spaces)
    description="""I'm an AI Development Assistant. I help with:
    
    ‚Ä¢ Analyzing business requirements for enterprise systems
    ‚Ä¢ Understanding large codebases and suggesting implementation approaches
    ‚Ä¢ Creating user stories with proper sizing and acceptance criteria  
    ‚Ä¢ Planning development work with domain expertise
    
    I specialize in complex business domains including finance, healthcare, and enterprise software.
    
    How can I help you with your system development today?""",
    
    instruction="""You are an AI assistant for development teams. You help analyze requirements,
    understand codebases, create stories, and plan implementations.
    
    Key capabilities:
    1. Requirements Analysis - Analyze complexity, identify domains, estimate effort
    2. Repository Analysis - Understand large codebases, find relevant files, suggest approaches
    3. Story Creation - Create properly formatted user stories with acceptance criteria
    4. Implementation Planning - Create detailed technical plans
    
    Always consider:
    - Business domain requirements and compliance needs
    - Security and regulatory requirements
    - Industry best practices and coding standards
    - Existing codebase patterns and architecture
    
    Be helpful, specific, and actionable in your responses.""",
    
    tools=[
        analyze_requirement,
        analyze_repository,
        create_user_story,
        get_workflow_status,
        plan_implementation,
        # Code search tools - only load if available
    ]
)

# Try to add code search tools if available
try:
    from hartford_agent.code_search_tools import (
        search_code,
        find_component,
        find_api_endpoint,
        find_database_query,
        find_configuration,
        explain_code_structure
    )
    
    # Add code search tools to the agent
    root_agent.tools.extend([
        search_code,
        find_component,
        find_api_endpoint,
        find_database_query,
        find_configuration,
        explain_code_structure
    ])
    print("‚úÖ Code search tools loaded successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Code search tools not available: {e}")